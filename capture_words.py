from asl_config import ASL_CLASSES, MODEL_DIR, EXTENTION, IMAGE_SIZE
import cv2
import numpy as np
import tensorflow as tf
import mediapipe as mp

# =========================
# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
# =========================
MODEL_PATH = f"{MODEL_DIR}/asl_multimodal_model.{EXTENTION}"
print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­è¾¼ä¸­: {MODEL_PATH}")
model = tf.keras.models.load_model(MODEL_PATH)
print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

# =========================
# MediaPipe åˆæœŸåŒ–
# =========================
mp_hands = mp.solutions.hands
mp_pose = mp.solutions.pose
mp_draw = mp.solutions.drawing_utils

hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)
pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5)

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def preprocess_image(frame, size=(64, 64)):
    """ç”»åƒã‚’CNNå…¥åŠ›ã‚µã‚¤ã‚ºã«å¤‰æ›"""
    img = cv2.resize(frame, size)
    return img.astype("float32") / 255.0

def extract_landmarks(results_hands, results_pose, w, h):
    """Pose(33ç‚¹) + Hands(æœ€å¤§2Ã—21ç‚¹) â†’ 225æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«"""
    all_pose, all_hands = [], []

    # Pose
    if results_pose.pose_landmarks:
        for lm in results_pose.pose_landmarks.landmark:
            all_pose.extend([lm.x, lm.y, lm.z])

    # Hands
    if results_hands.multi_hand_landmarks:
        for hand in results_hands.multi_hand_landmarks:
            for lm in hand.landmark:
                all_hands.extend([lm.x, lm.y, lm.z])

    combined = np.array(all_pose + all_hands, dtype=np.float32)
    if len(combined) < 225:
        combined = np.pad(combined, (0, 225 - len(combined)))
    else:
        combined = combined[:225]

    return combined

# =========================
# ã‚«ãƒ¡ãƒ©èµ·å‹•
# =========================
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

if not cap.isOpened():
    print("âŒ ã‚«ãƒ¡ãƒ©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    exit()
print("âœ… ã‚«ãƒ¡ãƒ©èµ·å‹•æˆåŠŸ")

# =========================
# ãƒ«ãƒ¼ãƒ—
# =========================
print("ğŸŸ¢ æ¨è«–é–‹å§‹ (Qã§çµ‚äº†)")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    h, w, _ = frame.shape
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # MediaPipeæ¨è«–
    results_hands = hands.process(rgb)
    results_pose = pose.process(rgb)

    # éª¨æ ¼æç”»
    if results_pose.pose_landmarks:
        mp_draw.draw_landmarks(frame, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)
    if results_hands.multi_hand_landmarks:
        for hand in results_hands.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)

    # å…¥åŠ›æ•´å½¢
    img_input = preprocess_image(frame, IMAGE_SIZE)
    skel_input = img_input.copy()  # ä»Šã¯åŒã˜æ˜ åƒã‚’ä½¿ã†ï¼ˆå¿…è¦ãªã‚‰åˆ¥æç”»ã«å¤‰æ›´å¯ï¼‰
    land_input = extract_landmarks(results_hands, results_pose, w, h)

    # æ¨è«–
    pred = model.predict([
        np.expand_dims(img_input, axis=0),
        np.expand_dims(skel_input, axis=0),
        np.expand_dims(land_input, axis=0)
    ], verbose=0)

    idx = int(np.argmax(pred))
    prob = float(np.max(pred))
    label = ASL_CLASSES[idx]

    # ç”»é¢è¡¨ç¤º
    cv2.putText(frame, f"{label} ({prob*100:.1f}%)",
                (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5,
                (0, 255, 0) if prob > 0.7 else (0, 165, 255), 3)

    cv2.imshow("ASL Prediction (Hands + Pose)", frame)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()